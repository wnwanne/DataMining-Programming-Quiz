{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter, defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "\n",
    "rcParams['figure.figsize'] = 14, 8\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview_df(_df):\n",
    "    print(_df.shape)\n",
    "    return _df.head()\n",
    "\n",
    "class Tokenizer:\n",
    "  \n",
    "    def clean(self, text):\n",
    "        no_html = BeautifulSoup(text).get_text()\n",
    "        clean = re.sub(\"[^a-z\\s]+\", \" \", no_html, flags=re.IGNORECASE)\n",
    "        return re.sub(\"(\\s+)\", \" \", clean)\n",
    "\n",
    " \n",
    "    def tokenize(self, text):\n",
    "        clean = self.clean(text).lower()\n",
    "        stopwords_en = stopwords.words(\"english\")\n",
    "        return [w for w in re.split(\"\\W+\", clean) if not w in stopwords_en]\n",
    "\n",
    "class MultinomialNaiveBayes:\n",
    "  \n",
    "    def __init__(self, classes, tokenizer):\n",
    "      self.tokenizer = tokenizer\n",
    "      self.classes = classes\n",
    "      \n",
    "    def group_by_class(self, X, y):\n",
    "      data = dict()\n",
    "      for c in self.classes:\n",
    "        data[c] = X[np.where(y == c)]\n",
    "      return data\n",
    "           \n",
    "    def fit(self, X, y):\n",
    "        self.n_class_items = {}\n",
    "        self.log_class_priors = {}\n",
    "        self.word_counts = {}\n",
    "        self.vocab = set()\n",
    "\n",
    "        n = len(X)\n",
    "        \n",
    "        grouped_data = self.group_by_class(X, y)\n",
    "        \n",
    "        for c, data in grouped_data.items():\n",
    "          self.n_class_items[c] = len(data)\n",
    "          self.log_class_priors[c] = math.log(self.n_class_items[c] / n)\n",
    "          self.word_counts[c] = defaultdict(lambda: 0)\n",
    "          \n",
    "          for text in data:\n",
    "            counts = Counter(self.tokenizer.tokenize(text))\n",
    "            for word, count in counts.items():\n",
    "                if word not in self.vocab:\n",
    "                    self.vocab.add(word)\n",
    "\n",
    "                self.word_counts[c][word] += count\n",
    "                \n",
    "        return self\n",
    "      \n",
    "    def laplace_smoothing(self, word, text_class, alpha=1000):\n",
    "      num = self.word_counts[text_class][word] + alpha\n",
    "      denom = self.n_class_items[text_class] + len(self.vocab)\n",
    "      return math.log(num / denom)\n",
    "      \n",
    "    def predict(self, X):\n",
    "        result = []\n",
    "        for text in X:\n",
    "          \n",
    "          class_scores = {c: self.log_class_priors[c] for c in self.classes}\n",
    "\n",
    "          words = set(self.tokenizer.tokenize(text))\n",
    "          for word in words:\n",
    "              if word not in self.vocab: continue\n",
    "\n",
    "              for c in self.classes:\n",
    "                \n",
    "                log_w_given_c = self.laplace_smoothing(word, c)\n",
    "                class_scores[c] += log_w_given_c\n",
    "                \n",
    "          result.append(max(class_scores, key=class_scores.get))\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'C:\\\\Users\\\\nwannew\\\\Documents\\\\Dev\\\\School\\\\intro to data mining\\\\aclImdb'\n",
    "labels = {'pos' : 1, 'neg': 0}\n",
    "\n",
    "train = pd.DataFrame()\n",
    "test = pd.DataFrame()\n",
    "\n",
    "train_folder = os.path.join(folder, 'train')\n",
    "test_folder = os.path.join(folder, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in ('pos', 'neg'):\n",
    "    path = os.path.join(train_folder, l)\n",
    "    for file in os.listdir(path):\n",
    "        with open(os.path.join(path, file), 'r', encoding='utf8') as infile:\n",
    "            txt = infile.read()\n",
    "        train = train.append([[txt, labels[l]]], ignore_index=True)\n",
    "train.columns = ['review', 'sentiment']\n",
    "preview_df(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in ('pos', 'neg'):\n",
    "    path = os.path.join(test_folder, l)\n",
    "    for file in os.listdir(path):\n",
    "        with open(os.path.join(path, file), 'r', encoding='utf8') as infile:\n",
    "            txt = infile.read()\n",
    "        test = test.append([[txt, labels[l]]], ignore_index=True)\n",
    "test.columns = ['review', 'sentiment']\n",
    "preview_df(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = sns.countplot(x='sentiment', data=train)\n",
    "f.set_title(\"Sentiment distribution\")\n",
    "f.set_xticklabels(['Negative', 'Positive'])\n",
    "plt.xlabel(\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(review for review in train.review)\n",
    "\n",
    "\n",
    "wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\", stopwords=stopwords.words(\"english\")).generate(text)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train['review'].values\n",
    "y = train['sentiment'].values\n",
    "  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNB = MultinomialNaiveBayes(classes=np.unique(y), \n",
    "                            tokenizer=Tokenizer()).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = MNB.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(y_test, y_hat)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"negative\", \"positive\"]\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"Blues\", fmt=\"d\", cbar=False, xticklabels=class_names, yticklabels=class_names)\n",
    "ax.xaxis.set_label_position('top')\n",
    "plt.tight_layout()\n",
    "plt.ylabel('Actual sentiment')\n",
    "plt.xlabel('Predicted sentiment');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do you think the accuracy suffers when Î± is too high or too low?\n",
    "\n",
    "The accuracy most likely suffers due to the scaling that takes place when alpha is either decreased/increased. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
