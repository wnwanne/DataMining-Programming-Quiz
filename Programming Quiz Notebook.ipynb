{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task:\n",
    "The task is to estimate appropriate parameters using the training data, and use it to predict reviews from the test data, and classify each of them as either positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from matplotlib import rc\n",
    "#from google.colab import drive\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter, defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from  sklearn.metrics  import accuracy_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "\n",
    "rcParams['figure.figsize'] = 14, 8\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview_df(_df):\n",
    "    print(_df.shape)\n",
    "    return df.head()\n",
    "\n",
    "class Tokenizer:\n",
    "    \n",
    "    def clean(self, text):\n",
    "        no_html = BeautifulSoup(text).get_text()\n",
    "        clean = re.sub(\"[^a-z\\s]+\", \" \", no_html, flags=re.IGNORECASE)\n",
    "        return re.sub(\"(\\s+)\", \" \", clean)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        clean = self.clean(text).lower()\n",
    "        stopwords_en = stopwords.words(\"english\")\n",
    "        return [w for w in re.split(\"\\W+\", clean) if not w in stopwords_en]\n",
    "\n",
    "class MultinomialNaiveBayes:\n",
    "    \n",
    "    def __init__(self, classes, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.classes = classes\n",
    "    \n",
    "    def group_by_class(self, X, y):\n",
    "        data = dict()\n",
    "        for c in self.classes:\n",
    "            data[c] = X[np.where(y == c)]\n",
    "        return data\n",
    "           \n",
    "    def fit(self, X, y):\n",
    "        self.n_class_items = {}\n",
    "        self.log_class_priors = {}\n",
    "        self.word_counts = {}\n",
    "        self.vocab = set()\n",
    "\n",
    "        n = len(X)\n",
    "        \n",
    "        grouped_data = self.group_by_class(X, y)\n",
    "        \n",
    "        for c, data in grouped_data.items():\n",
    "            self.n_class_items[c] = len(data)\n",
    "            self.log_class_priors[c] = math.log(self.n_class_items[c] / n)\n",
    "            self.word_counts[c] = defaultdict(lambda: 0)\n",
    "          \n",
    "            for text in data:\n",
    "                counts = Counter(self.tokenizer.tokenize(text))\n",
    "                for word, count in counts.items():\n",
    "                    if word not in self.vocab:\n",
    "                        self.vocab.add(word)\n",
    "\n",
    "                    self.word_counts[c][word] += count\n",
    "        return self\n",
    "      \n",
    "    def laplace_smoothing(self, word, text_class):\n",
    "        num = self.word_counts[text_class][word] + 1\n",
    "        denom = self.n_class_items[text_class] + len(self.vocab)\n",
    "        return math.log(num / denom)\n",
    "      \n",
    "    def predict(self, X):\n",
    "        result = []\n",
    "        for text in X:\n",
    "            \n",
    "            class_scores = {c: self.log_class_priors[c] for c in self.classes}\n",
    "\n",
    "            words = set(self.tokenizer.tokenize(text))\n",
    "            for word in words:\n",
    "                if word not in self.vocab: continue\n",
    "\n",
    "                for c in self.classes:\n",
    "                \n",
    "                    log_w_given_c = self.laplace_smoothing(word, c)\n",
    "                    class_scores[c] += log_w_given_c\n",
    "                \n",
    "            result.append(max(class_scores, key=class_scores.get))\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\Owner\\\\Dev\\\\Python\\\\Data Mining\\\\ProgrammingQuizData\\\\aclImdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'C:\\\\Users\\\\nwannew\\\\Documents\\\\Dev\\\\School\\\\intro to data mining\\\\aclImdb'\n",
    "labels = {'pos' : 1, 'neg': 0}\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = os.path.join(folder, f, l)\n",
    "        for file in os.listdir(path):\n",
    "            with open(os.path.join(path, file), 'r', encoding='utf8') as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[l]]], ignore_index=True)\n",
    "df.columns = ['review', 'sentiment']\n",
    "preview_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = sns.countplot(x='sentiment', data=train_df)\n",
    "f.set_title(\"Training Sentiment distribution\")\n",
    "f.set_xticklabels(['Negative', 'Positive'])\n",
    "plt.xlabel(\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = sns.countplot(x='sentiment', data=test_df)\n",
    "f.set_title(\"Testing Sentiment distribution\")\n",
    "f.set_xticklabels(['Negative', 'Positive'])\n",
    "plt.xlabel(\"\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the first task is to go through all the files in the ‘train’ folder, and construct the vocabulary V of all unique words. Please ignore all the stop-words. The words from each file (both in training and testing phase) must be extracted by splitting the raw text only with whitespace characters and converting them to lowercase characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = df.review.str.cat(sep=' ')\n",
    "#function to split text into word\n",
    "tokens = tokenize(reviews)\n",
    "vocabulary = set(tokens)\n",
    "print(len(vocabulary))\n",
    "frequency_dist = nltk.FreqDist(tokens)\n",
    "sorted(frequency_dist,key=frequency_dist.__getitem__, reverse=True)[0:50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [w for w in tokens if not w in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud().generate_from_frequencies(frequency_dist)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.loc[:24999, 'review'].values\n",
    "y_train = df.loc[:24999, 'sentiment'].values\n",
    "X_test = df.loc[25000:, 'review'].values\n",
    "y_test = df.loc[25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "train_vectors = vectorizer.fit_transform(X_train)\n",
    "test_vectors = vectorizer.transform(X_test)\n",
    "print(train_vectors.shape, test_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1.0\n",
    "\n",
    "clf = MultinomialNB(alpha=alpha,fit_prior=True).fit(train_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(test_vectors)\n",
    "print(accuracy_score(y_test,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = os.path.join(path2, 'test')\n",
    "train_path = os.path.join(path2, 'train')\n",
    "pos_test = os.path.join(test_path, 'pos')\n",
    "neg_test = os.path.join(test_path, 'neg')\n",
    "pos_train = os.path.join(train_path, 'pos')\n",
    "neg_train = os.path.join(train_path, 'neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train_list = []\n",
    "for file in os.listdir(pos_train):\n",
    "    file_path = os.path.join(pos_train, file)\n",
    "    with open(file_path, 'r', encoding='utf8') as n:\n",
    "        line = n.read()\n",
    "        pos_train_list.extend(tokenize(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_train_list = []\n",
    "for file in os.listdir(pos_train):\n",
    "    file_path = os.path.join(pos_train, file)\n",
    "    with open(file_path, 'r', encoding='utf8') as n:\n",
    "        line = n.read()\n",
    "        neg_train_list.extend(tokenize(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_test_list = []\n",
    "for file in os.listdir(pos_train):\n",
    "    file_path = os.path.join(pos_train, file)\n",
    "    with open(file_path, 'r', encoding='utf8') as n:\n",
    "        line = n.read()\n",
    "        pos_test_list.extend(tokenize(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_test_list = []\n",
    "for file in os.listdir(pos_train):\n",
    "    file_path = os.path.join(pos_train, file)\n",
    "    with open(file_path, 'r', encoding='utf8') as n:\n",
    "        line = n.read()\n",
    "        neg_test_list.extend(tokenize(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = list(itertools.chain(pos_test_list,neg_test_list,pos_train_list,neg_train_list))\n",
    "unique_vocab_list = set(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_list = set(vocab_list)\n",
    "len(unique_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to get counts of each individual words for the positive and the negative classes\n",
    "separately, to get P(word|class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts per values\n",
    "pos_dict_counts = Counter(pos_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts per value\n",
    "neg_dict_counts = Counter(neg_train_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the log-posterior (un-normalized), which is given by log(P(X|Y )P(Y )), for both the classes with laplace smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_logs = {}\n",
    "for c, data in pos_dict_counts.items():\n",
    "    logs[c] = math.log(data + 1 / (len(pos_train_list) + 1) +\n",
    "                       len(pos_train_list)/(len(pos_train_list)+len(neg_train_list)))\n",
    "neg_logs = {}\n",
    "for c, data in neg_dict_counts.items():\n",
    "    logs[c] = math.log(data + 1 / (len(neg_train_list) + 1) + \n",
    "                       len(neg_train_list)/(len(pos_train_list)+len(neg_train_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
