{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task:\n",
    "The task is to estimate appropriate parameters using the training data, and use it to predict reviews from the test data, and classify each of them as either positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Owner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import itertools\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    no_html = BeautifulSoup(text).get_text()\n",
    "    clean = re.sub(\"[^a-z\\s]+\",\" \", no_html, flags=re.IGNORECASE)\n",
    "    return re.sub(\"(\\s+)\", \" \", clean)\n",
    "\n",
    "def tokenize(text):\n",
    "    cleaned = clean(text).lower()\n",
    "    stopwords_en = set(stopwords.words('english'))\n",
    "    return [w for w in re.split('\\W+', cleaned) if not w in stopwords_en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\Owner\\\\Dev\\\\Python\\\\Data Mining\\\\ProgrammingQuizData\\\\aclImdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = os.path.join(path, 'test')\n",
    "train_path = os.path.join(path, 'train')\n",
    "pos_test = os.path.join(test_path, 'pos')\n",
    "neg_test = os.path.join(test_path, 'neg')\n",
    "pos_train = os.path.join(train_path, 'pos')\n",
    "neg_train = os.path.join(train_path, 'neg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the first task is to go through all the files in the ‘train’ folder, and construct the vocabulary V of all unique words. Please ignore all the stop-words. The words from each file (both in training and testing phase) must be extracted by splitting the raw text only with whitespace characters and converting them to lowercase characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train_list = []\n",
    "for file in os.listdir(pos_train):\n",
    "    file_path = os.path.join(pos_train, file)\n",
    "    with open(file_path, 'r', encoding='utf8') as n:\n",
    "        line = n.read()\n",
    "        pos_train_list.extend(tokenize(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_train_list = []\n",
    "for file in os.listdir(pos_train):\n",
    "    file_path = os.path.join(pos_train, file)\n",
    "    with open(file_path, 'r', encoding='utf8') as n:\n",
    "        line = n.read()\n",
    "        neg_train_list.extend(tokenize(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_test_list = []\n",
    "for file in os.listdir(pos_train):\n",
    "    file_path = os.path.join(pos_train, file)\n",
    "    with open(file_path, 'r', encoding='utf8') as n:\n",
    "        line = n.read()\n",
    "        pos_test_list.extend(tokenize(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_test_list = []\n",
    "for file in os.listdir(pos_train):\n",
    "    file_path = os.path.join(pos_train, file)\n",
    "    with open(file_path, 'r', encoding='utf8') as n:\n",
    "        line = n.read()\n",
    "        neg_test_list.extend(tokenize(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = list(itertools.chain(pos_test_list,neg_test_list,pos_train_list,neg_train_list))\n",
    "unique_vocab_list = set(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54991"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_list = set(vocab_list)\n",
    "len(unique_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to get counts of each individual words for the positive and the negative classes\n",
    "separately, to get P(word|class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts per values\n",
    "pos_dict_counts = Counter(pos_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts per value\n",
    "neg_dict_counts = Counter(neg_train_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the log-posterior (un-normalized), which is given by log(P(X|Y )P(Y )), for both the classes with laplace smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_logs = {}\n",
    "for c, data in pos_dict_counts.items():\n",
    "    logs[c] = math.log(data + 1 / (len(pos_train_list) + 1) +\n",
    "                       len(pos_train_list)/(len(pos_train_list)+len(neg_train_list)))\n",
    "neg_logs = {}\n",
    "for c, data in neg_dict_counts.items():\n",
    "    logs[c] = math.log(data + 1 / (len(neg_train_list) + 1) + \n",
    "                       len(neg_train_list)/(len(pos_train_list)+len(neg_train_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
